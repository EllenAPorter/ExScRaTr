\section{Limitations of Current Ray Tracing Algorithms}

Traditional ray tracing algorithms are embarrassingly parallel as no ray depends on any other ray. The data needed by each individual ray however varies widely as its path is traced. Acceleration structures, such as k-d trees have been developed to increase ray tracing performance. As the data scales up however, it is no longer possible to store an entire data set in an acceleration structure in shared memory. One solution is to implement data decomposition. Each node on a distributed system is then responsible for a subset of the domain. Primary rays and secondary rays are then communicated across nodes as the algorithm executes. These types of models typically rely on expensive preprocessing steps that help to balance both the data distribution and rendering work evenly across nodes ~\cite{navratil2014dynamic}.  

Load balancing, a significant bottle neck of todayâ€™s systems may not be when we look towards exascale systems.  The proposed smarter programming models and runtimes will allow for scheduling and data movement decisions to be made at runtime which will help reduce imbalance in a system.  Data and computation can be dynamically migrated off of over worked nodes assuming a properly sized granularity for tasks and data. 


%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "main"
%%% End: 
