\section{Reaching Exascale}

For the past two decades high performance computing (HPC) progression has been driven by Moore's law which states microprocessor performance and memory chip density increase exponentially over time. Until 2004, performance of single-core microprocessors increased as predicted as a result of smaller and faster transistors being developed. In 2004 this advancement trend shifted as we reached an inflection point caused by a chip’s power dissipation ~\cite{kogge2013exascale}. Unable to sufficiently and inexpensively cool a chip, chip designers looked for other ways to increase performance. This came in the form of multi-core processors which are now the building blocks of many HPC systems.

The introduction of multi-core processors on a single node of a cluster caused a shift in parallel application design. Programs using the standard Message Passing Interface (MPI) library ~\cite{Snir:1998:MCR:552013} could not exploit the parallelism on a single node without a rewrite of the underlying algorithms. This resulted in the emergence of hybrid systems that mix MPI and the Open Multi-Processing (OpenMP) ~\cite{openmp08} libraries.  Each node would execute OpenMP program controlled by an MPI process. The OpenMP program then used a fixed number of threads to execute a single work-sharing construct such as a parallel loop ~\cite{gropp2013programming}.

Although the exact exascale ecosystem is unknown, research suggests that data movement will overtake computation as the dominant cost in the system.  This results from the primary growth for parallelism being on chip, with some predictions suggesting hundreds or even thousands of cores per chip die. As a result we will see a higher available bandwidth on chip along with lower latencies for communication within a node.  The lower overhead within a chip provides a significant incentive to develop “communication avoiding” algorithms.  Communication avoidance can occur by re-compute values instead of communicating results when possible as well as by dividing the algorithm into functional partitions where each node solves a different part of an overall application with limited communication necessary between the components.
Many of our current programming models lack the semantics necessary to implement communication avoiding algorithms.  As a result new languages with additional semantics are being proposed for exascale systems.  A common theme among these languages is the ability to statically declare data dependencies and data locality information.  These additional details can then be used by the runtime to aid in scheduling and preemptive data movement.

\subsection{Task-based programming models}

Implemented in several different languages, one type of programming model that may map well onto exascale systems are task-based models.  Task-based models tend to be declarative, an application is broken down into chunks of work and the inputs and outputs to that work are declared in the language semantics.  The explicit data dependencies allow the runtime to optimally schedule and execute the tasks, or chunks of work, in the application.  Execution can often be further improved by the implementation of a secondary file separate from the program that provides hints to the runtime.  The key difference between many task-based models and more traditional programming models is the movement from compute-centric to data-centric application design.  Algorithms are designed around the data a task needs to execute and the data it will produce rather than designed around the computation.

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "main"
%%% End: 
