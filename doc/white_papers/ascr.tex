\documentclass[12pt]{article}

% RRL: I think "geometry" takes care of these more consistently.
% \textwidth=7in
% \textheight=9.5in
% \topmargin=-1in
% \headheight=0in
% \headsep=.5in
% \hoffset  -.85in

\usepackage{geometry}
\geometry{verbose,letterpaper,
  tmargin=0.5in,bmargin=0.8in,lmargin=1in,rmargin=1in,
  headheight=0in,headsep=0in,footskip=0.5in}

\usepackage{graphicx}
\usepackage{multirow}
\usepackage{setspace}
\usepackage{array}
\usepackage{hyperref}


\title{Exascale Ray Tracing: A White Paper}
\author{Ellen A. Porter\footnote{
    Battelle Pacific Northwest National Laboratory 
    and Washington State University;
    Program in Engineering and Computer Science
    (\texttt{ellen.porter@pnnl.gov})
  }
\hspace{1in} Robert R. Lewis\footnote{
    Washington State University;
    Program in Engineering and Computer Science
    (\texttt{bobl@tricity.wsu.edu})
  }
}
\frenchspacing

\begin{document}
\maketitle
\section*{Introduction}

Exascale computers, being defined as being capable of performing at
least one exaflop (10$^{18}$ floating point operations per second),
are anticipated by 2018. This is three orders of magnitude greater
than current supercomputers.

Scaling applications to exascale is a task that will take significant
programming effort. Conclusions from several DOE-funded research
projects suggest that at exascale we will see a shift in programming
models away from traditional ``bulk synchronous'' (aka,
``machine-level'') parallel computing models, such as MPI (possibly
combined with OpenMP), and towards high level ``task-based''
programming models. In these, the mapping of execution to hardware is
based upon a developer-designated logical partitioning of the program.

Task-based models rely more heavily on the runtime for optimization,
especially load balancing, and fault tolerance, which is expected to
be a necessity of life on exascale computers. We are proposing to
investigate the implications of these models for ray tracing.

Ray tracing is a widely-used algorithm. Beyond its well-recognized
applications in entertainment, it has also been used to study radio
signal propagation, ocean acoustics, optical design, seismology,
plasma physics, and the design of nuclear facilities, to name a few
applications. Often, ray tracing is used for data visualization. Manta
(University of Utah) and pvOSPRay (Texas Advanced Computing Center)
are two ray tracing plugins developed for core HPC visualization tools
such as VisIt and ParaView. Both of them work on distributed systems
but may not scale to exascale in their current form.

\section*{Proposed Work}

We propose the development of a generic task-based ray tracing system
that will run on both today's hardware and tomorrow's exascale
machines. As ray tracing in general is not a new field we will
implement a system that takes advantage of existing highly optimized
open source ray tracing engines. It may be integrated into a
visualization system or it may run as a stand-alone program.

Our general approach is to partition the scene volume into voxels and
the system will assign varying amounts of computing power to them. In
keeping with the expectations for exascale computing, the assignment
of compute power to data will be the responsibility of the runtime
guided by our partition and the tuning specification.

\subsection*{Phase 1: Building a Bulk Synchronous System}

We will first design and build a cluster that is capable of performing
bulk synchronous ray tracing. This may take the form of an
implementation of OSPRay running on our clusters. OSPRay is an open
source ray tracing rendering engine that allows for distributed (i.e.
bulk synchronous) computation. It is built on Intel's open source,
state-of-the-art Embree ray tracing kernel and is the basis of the
aforementioned pvOSPRay.

Future work may include integrating with OptiX, NVidia's ray tracing
kernel that runs on GPUs as well as extending the algorithm to
dynamically choose at runtime which library or libraries to use based
on the current runtime system.

\subsection*{Phase 2: Adapting to a Task-Based Model}

As with other applications, the shift towards exascale will take us
towards task-based programming models as it changes the underlying
hardware. This has implications for ray tracing and is what we will
explore in the second phase of our work.

For example, many existing distributed ray tracers duplicate data
across nodes. As the size of memory per node is projected to decline
significantly at exascale, this key design assumption will hinder the
performance of current algorithms.

We will therefore redesign the current algorithms and implement a
solution using a task-based model that will perform on today's
architecture as well as next generation's. Initially this will be
Intel's Concurrent Collections (CnC), which will be its first use for
graphics.

\subsection*{Phase 3: Extending Performance and Functionality}

With the base system in place we will then extend our work in two key
ways: optimizing performance and producing a wider variety of effects.
On many proposed task-based programming models, including CnC, a
separate ``tuning specification'' can provide hints to the runtime for
scheduling and data movement to increase performance. We will
implement and test this tuning specification for a variety of
scenarios.

Currently, load balancing is an important factor in the efficient use
of parallel computing. On exascale hardware, it is anticipated that
getting the tuning specification right will take the place of load
balancing heuristics.

We will then add more ray tracing functionality in the form of
contemporary features such as global illumination, volume propagation,
path tracing, subsurface scattering, time dependence, support for
multiple displays, and virtual reality.

\section*{Conclusion}

In the past, software developers have not adequately anticipated
advances in hardware. Software optimized for one generation of
hardware turns out to be less-than-optimal for the next. As such, the
new hardware is under-utilized.

Exascale is coming. We propose to develop a framework for ray tracing
that, while running on existing hardware, will transition easily to
exascale systems.

\end{document}